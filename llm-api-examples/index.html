<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>LLM Examples Introduction &mdash; tensorrt_llm  documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=19f00094" />

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../_static/jquery.js?v=5d32c60e"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../_static/documentation_options.js?v=5929fcd5"></script>
        <script src="../_static/doctools.js?v=888ff710"></script>
        <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Common Customizations" href="customization.html" />
    <link rel="prev" title="Building from Source Code on Windows" href="../installation/build-from-source-windows.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            tensorrt_llm
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick-start-guide.html">Quick Start Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../key-features.html">Key Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="../release-notes.html">Release Notes</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Installation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../installation/linux.html">Installing on Linux</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation/build-from-source-linux.html">Building from Source Code on Linux</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation/windows.html">Installing on Windows</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation/build-from-source-windows.html">Building from Source Code on Windows</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">LLM API Examples</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">LLM Examples Introduction</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#supported-models">Supported Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="#model-preparation">Model Preparation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#hugging-face-hub">Hugging Face Hub</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#local-hugging-face-models">Local Hugging Face Models</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#from-tensorrt-llm-engine">From TensorRT-LLM Engine</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="customization.html">Common Customizations</a></li>
<li class="toctree-l1"><a class="reference internal" href="llm_api_examples.html">Examples</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">LLM API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../llm-api/index.html">API Reference</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Definition API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../python-api/tensorrt_llm.layers.html">Layers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../python-api/tensorrt_llm.functional.html">Functionals</a></li>
<li class="toctree-l1"><a class="reference internal" href="../python-api/tensorrt_llm.models.html">Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../python-api/tensorrt_llm.plugin.html">Plugin</a></li>
<li class="toctree-l1"><a class="reference internal" href="../python-api/tensorrt_llm.quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../python-api/tensorrt_llm.runtime.html">Runtime</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">C++ API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../_cpp_gen/executor.html">Executor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../_cpp_gen/runtime.html">Runtime</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Command-Line Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../commands/trtllm-build.html">trtllm-build</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Architecture</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../architecture/overview.html">TensorRT-LLM Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../architecture/core-concepts.html">Model Definition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../architecture/core-concepts.html#compilation">Compilation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../architecture/core-concepts.html#runtime">Runtime</a></li>
<li class="toctree-l1"><a class="reference internal" href="../architecture/core-concepts.html#multi-gpu-and-multi-node-support">Multi-GPU and Multi-Node Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="../architecture/checkpoint.html">TensorRT-LLM Checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../architecture/workflow.html">TensorRT-LLM Build Workflow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../architecture/add-model.html">Adding a Model</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Advanced</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../advanced/gpt-attention.html">Multi-Head, Multi-Query, and Group-Query Attention</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/gpt-runtime.html">C++ GPT Runtime</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/graph-rewriting.html">Graph Rewriting Module</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/batch-manager.html">The Batch Manager in TensorRT-LLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/inference-request.html">Inference Request</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/inference-request.html#responses">Responses</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/lora.html">Run gpt-2b + LoRA using GptManager / cpp runtime</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/expert-parallelism.html">Expert Parallelism in TensorRT-LLM</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Performance</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../performance/perf-overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../performance/perf-best-practices.html">Best Practices for Tuning the Performance of TensorRT-LLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../performance/perf-analysis.html">Performance Analysis</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../reference/troubleshooting.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../reference/support-matrix.html">Support Matrix</a></li>
<li class="toctree-l1"><a class="reference internal" href="../reference/precision.html">Numerical Precision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../reference/memory.html">Memory Usage of TensorRT-LLM</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Blogs</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../blogs/H100vsA100.html">H100 has 4.6x A100 Performance in TensorRT-LLM, achieving 10,000 tok/s at 100ms to first token</a></li>
<li class="toctree-l1"><a class="reference internal" href="../blogs/H200launch.html">H200 achieves nearly 12,000 tokens/sec on Llama2-13B with TensorRT-LLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../blogs/Falcon180B-H200.html">Falcon-180B on a single H200 GPU with INT4 AWQ, and 6.7x faster Llama-70B over A100</a></li>
<li class="toctree-l1"><a class="reference internal" href="../blogs/quantization-in-TRT-LLM.html">Speed up inference with SOTA quantization techniques in TRT-LLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../blogs/XQA-kernel.html">New XQA-kernel provides 2.4x more Llama-70B throughput within the same latency budget</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">tensorrt_llm</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">LLM Examples Introduction</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/llm-api-examples/index.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="llm-examples-introduction">
<h1>LLM Examples Introduction<a class="headerlink" href="#llm-examples-introduction" title="Link to this heading"></a></h1>
<p>Here is a simple example to show how to use the LLM with TinyLlama.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="kn">from</span> <span class="nn">tensorrt_llm</span> <span class="kn">import</span> <span class="n">LLM</span><span class="p">,</span> <span class="n">SamplingParams</span>
<span class="linenos"> 2</span>
<span class="linenos"> 3</span><span class="n">prompts</span> <span class="o">=</span> <span class="p">[</span>
<span class="linenos"> 4</span>    <span class="s2">&quot;Hello, my name is&quot;</span><span class="p">,</span>
<span class="linenos"> 5</span>    <span class="s2">&quot;The president of the United States is&quot;</span><span class="p">,</span>
<span class="linenos"> 6</span>    <span class="s2">&quot;The capital of France is&quot;</span><span class="p">,</span>
<span class="linenos"> 7</span>    <span class="s2">&quot;The future of AI is&quot;</span><span class="p">,</span>
<span class="linenos"> 8</span><span class="p">]</span>
<span class="linenos"> 9</span><span class="n">sampling_params</span> <span class="o">=</span> <span class="n">SamplingParams</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">top_p</span><span class="o">=</span><span class="mf">0.95</span><span class="p">)</span>
<span class="linenos">10</span>
<span class="linenos">11</span><span class="n">llm</span> <span class="o">=</span> <span class="n">LLM</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;TinyLlama/TinyLlama-1.1B-Chat-v1.0&quot;</span><span class="p">)</span>
<span class="linenos">12</span>
<span class="linenos">13</span><span class="n">outputs</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">prompts</span><span class="p">,</span> <span class="n">sampling_params</span><span class="p">)</span>
<span class="linenos">14</span>
<span class="linenos">15</span><span class="c1"># Print the outputs.</span>
<span class="linenos">16</span><span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">:</span>
<span class="linenos">17</span>    <span class="n">prompt</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">prompt</span>
<span class="linenos">18</span>    <span class="n">generated_text</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">text</span>
<span class="linenos">19</span>    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Prompt: </span><span class="si">{</span><span class="n">prompt</span><span class="si">!r}</span><span class="s2">, Generated text: </span><span class="si">{</span><span class="n">generated_text</span><span class="si">!r}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>The LLM API can be used for both offline or online usage. See more examples of the LLM API here:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://nvidia.github.io/TensorRT-LLM/llm-api-examples/llm_generate.html">LLM Generate</a></p></li>
<li><p><a class="reference external" href="https://nvidia.github.io/TensorRT-LLM/llm-api-examples/llm_generate_distributed.html">LLM Generate Distributed</a></p></li>
<li><p><a class="reference external" href="https://nvidia.github.io/TensorRT-LLM/llm-api-examples/llm_generate_async.html">LLM Generate Async</a></p></li>
<li><p><a class="reference external" href="https://nvidia.github.io/TensorRT-LLM/llm-api-examples/llm_generate_async_streaming.html">LLM Generate Async Streaming</a></p></li>
<li><p><a class="reference external" href="https://nvidia.github.io/TensorRT-LLM/llm-api-examples/llm_quantization.html">LLM Quantization</a></p></li>
<li><p><a class="reference external" href="https://nvidia.github.io/TensorRT-LLM/llm-api-examples/llm_auto_parallel.html">LLM Auto Parallel</a></p></li>
</ul>
<p>For more details on how to fully utilize this API, check out:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://nvidia.github.io/TensorRT-LLM/llm-api-examples/customization.html">Common customizations</a></p></li>
<li><p><a class="reference external" href="https://nvidia.github.io/TensorRT-LLM/llm-api/index.html">LLM API Reference</a></p></li>
</ul>
<section id="supported-models">
<h2>Supported Models<a class="headerlink" href="#supported-models" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p>Llama (including variants Mistral, Mixtral, InternLM)</p></li>
<li><p>GPT (including variants Starcoder-1/2, Santacoder)</p></li>
<li><p>Gemma-1/2</p></li>
<li><p>Phi-1/2/3</p></li>
<li><p>ChatGLM (including variants glm-10b, chatglm, chatglm2, chatglm3, glm4)</p></li>
<li><p>QWen-1/1.5/2</p></li>
<li><p>Falcon</p></li>
<li><p>Baichuan-1/2</p></li>
<li><p>GPT-J</p></li>
</ul>
</section>
<section id="model-preparation">
<h2>Model Preparation<a class="headerlink" href="#model-preparation" title="Link to this heading"></a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">LLM</span></code> class supports input from any of following:</p>
<ol class="arabic simple">
<li><p><strong>Hugging Face Hub</strong>: triggers a download from the Hugging Face model hub, such as <code class="docutils literal notranslate"><span class="pre">TinyLlama/TinyLlama-1.1B-Chat-v1.0</span></code>.</p></li>
<li><p><strong>Local Hugging Face models</strong>: uses a locally stored Hugging Face model.</p></li>
<li><p><strong>Local TensorRT-LLM engine</strong>: built by <code class="docutils literal notranslate"><span class="pre">trtllm-build</span></code> tool or saved by the Python LLM API.</p></li>
</ol>
<p>Any of these formats can be used interchangeably with the LLM(model=<any-model-path>) constructor.
The following sections how to use get these different formats for the LLM API.</p>
<section id="hugging-face-hub">
<h3>Hugging Face Hub<a class="headerlink" href="#hugging-face-hub" title="Link to this heading"></a></h3>
<p>Using the hugging face hub is as simple as specifying the repo name in the LLM constructor</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">llm</span> <span class="o">=</span> <span class="n">LLM</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;TinyLlama/TinyLlama-1.1B-Chat-v1.0&quot;</span><span class="p">)</span>
</pre></div>
</div>
<section id="local-hugging-face-models">
<h4>Local Hugging Face Models<a class="headerlink" href="#local-hugging-face-models" title="Link to this heading"></a></h4>
<p>Given the popularity of the Hugging Face model hub, the API supports the Hugging Face format as one of the starting points.
To use the API with Llama 3.1 models, download the model from the <a class="reference external" href="https://huggingface.co/meta-llama/Meta-Llama-3.1-8B">Meta Llama 3.1 8B model page</a> by using the following command:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">git lfs install</span>
<span class="go">git clone https://huggingface.co/meta-llama/Meta-Llama-3.1-8B</span>
</pre></div>
</div>
<p>After the model downloading finished, we can load the model as below.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">llm</span> <span class="o">=</span> <span class="n">LLM</span><span class="p">(</span><span class="n">model</span><span class="o">=&lt;</span><span class="n">path_to_meta_llama_from_hf</span><span class="o">&gt;</span><span class="p">)</span>
</pre></div>
</div>
<p>Note that using this model is subject to a <a class="reference external" href="https://ai.meta.com/resources/models-and-libraries/llama-downloads/">particular</a> license. Agree to the terms and <a class="reference external" href="https://huggingface.co/meta-llama/Meta-Llama-3-8B?clone=true">authenticate with HuggingFace</a> to begin the download.</p>
</section>
</section>
<section id="from-tensorrt-llm-engine">
<h3>From TensorRT-LLM Engine<a class="headerlink" href="#from-tensorrt-llm-engine" title="Link to this heading"></a></h3>
<p>There are two ways to build the TensorRT-LLM engine:</p>
<ol class="arabic">
<li><p>You can build the TensorRT-LLM engine from the Hugging Face model directly with the <a class="reference external" href="https://github.com/NVIDIA/TensorRT-LLM/blob/main/setup.py#L126"><code class="docutils literal notranslate"><span class="pre">trtllm-build</span></code></a> tool and then save the engine to disk for later use.
Refer to the <a class="reference external" href="https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/llama">README</a> in the <a class="reference external" href="https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/llama"><code class="docutils literal notranslate"><span class="pre">examples/llama</span></code></a> repository on GitHub.</p>
<p>After the engine building is finished, we can load the model as below.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">llm</span> <span class="o">=</span> <span class="n">LLM</span><span class="p">(</span><span class="n">model</span><span class="o">=&lt;</span><span class="n">path_to_trt_engine</span><span class="o">&gt;</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Use an <code class="docutils literal notranslate"><span class="pre">LLM</span></code> instance to create the engine and persist to local disk:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">llm</span> <span class="o">=</span> <span class="n">LLM</span><span class="p">(</span><span class="o">&lt;</span><span class="n">model</span><span class="o">-</span><span class="n">path</span><span class="o">&gt;</span><span class="p">)</span>

<span class="c1"># Save engine to local disk</span>
<span class="n">llm</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="o">&lt;</span><span class="n">engine</span><span class="o">-</span><span class="nb">dir</span><span class="o">&gt;</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ol>
<p>The engine can be reloaded like above.</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../installation/build-from-source-windows.html" class="btn btn-neutral float-left" title="Building from Source Code on Windows" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="customization.html" class="btn btn-neutral float-right" title="Common Customizations" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
<jinja2.runtime.BlockReference object at 0x7fea0e07a3e0>

<div class="footer">
    <p>
        Copyright © 2024 NVIDIA Corporation
    </p>
    <p>
        <a class="Link" href="https://www.nvidia.com/en-us/about-nvidia/privacy-policy/" target="_blank" rel="noopener"
            data-cms-ai="0">Privacy Policy</a> |
        <a class="Link" href="https://www.nvidia.com/en-us/about-nvidia/privacy-center/" target="_blank" rel="noopener"
            data-cms-ai="0">Manage My Privacy</a> |
        <a class="Link" href="https://www.nvidia.com/en-us/preferences/start/" target="_blank" rel="noopener"
            data-cms-ai="0">Do Not Sell or Share My Data</a> |
        <a class="Link" href="https://www.nvidia.com/en-us/about-nvidia/terms-of-service/" target="_blank"
            rel="noopener" data-cms-ai="0">Terms of Service</a> |
        <a class="Link" href="https://www.nvidia.com/en-us/about-nvidia/accessibility/" target="_blank" rel="noopener"
            data-cms-ai="0">Accessibility</a> |
        <a class="Link" href="https://www.nvidia.com/en-us/about-nvidia/company-policies/" target="_blank"
            rel="noopener" data-cms-ai="0">Corporate Policies</a> |
        <a class="Link" href="https://www.nvidia.com/en-us/product-security/" target="_blank" rel="noopener"
            data-cms-ai="0">Product Security</a> |
        <a class="Link" href="https://www.nvidia.com/en-us/contact/" target="_blank" rel="noopener"
            data-cms-ai="0">Contact</a>
    </p>
</div>


  </div>

   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>