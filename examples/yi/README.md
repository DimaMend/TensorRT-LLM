# Yi

This document shows how to build and run a model [Yi](https://github.com/01-ai/Yi) (including [Yi-6B](https://huggingface.co/01-ai/Yi-6B) / [Yi-34B](https://huggingface.co/01-ai/Yi-34B)) in TensorRT-LLM, both tensor parallelism acceleration and pipeline parallelism acceleration are supported.

## Overview

The TensorRT-LLM model Yi implementation can be found in [tensorrt_llm/models/yi/model.py](../../tensorrt_llm/models/yi/model.py). The TensorRT-LLM Yi example code is located in [`examples/yi`](./). 
Main files in that folder::

 * [`build.py`](./build.py) to build the [TensorRT](https://developer.nvidia.com/tensorrt) engine(s) needed to run the Yi model
 * [`run.py`](./run.py) to run the inference on an input text or an input file

## Support Matrix
  * BF16
  * FP16
  * FP32

## Usage

The TensorRT-LLM Yi example code locates at [examples/yi](./). It takes Huggingface(HF) model weights as input, and builds the corresponding TensorRT engines. The number of TensorRT engines depends on the number of GPUs used to run inference.

### Build TensorRT engine(s)

First you need to install dependencies :
```bash
pip install -r requirements.txt
```
To specify the HF Yi checkpoint path, you can download it from some online repositories, like Huggingface([Yi-6B](https://huggingface.co/01-ai/Yi-6B) / [Yi-34B](https://huggingface.co/01-ai/Yi-34B)) or Modelscope([Yi-6B](https://modelscope.cn/models/01ai/Yi-6B/summary) / [Yi-34B](https://modelscope.cn/models/01ai/Yi-34B/summary))

TensorRT-LLM Yi builds TensorRT engine(s) from HF checkpoint. If no checkpoint directory is specified, TensorRT-LLM will build engine(s) with dummy weights.

Normally `build.py` only requires single GPU, but if you've already got all the GPUs needed while inferencing, you could enable parallelly building to make the engine building process faster by adding `--parallel_build` argument. Please note that currently `parallel_build` feature only supports single node.

Here're some examples :

```bash
# Enable the special TensorRT-LLM GPT Attention plugin (--use_gpt_attention_plugin) to increase runtime performance.
# Try use_gemm_plugin to prevent accuracy issue.

# Build Yi-6B model using a single GPU with bfloat16 datatype
python build.py \
    --model_dir 01-ai/Yi-6B \
    --dtype bfloat16 \
    --log_level info \
    --output_dir trtllm_models/yi-6b_tp1_pp1 \
    --remove_input_padding \
    --use_gpt_attention_plugin bfloat16 \
    --use_gemm_plugin bfloat16 \
    --enable_context_fmha \
    --use_fused_mlp

# Build the Yi-34B model using a 4 GPU devices with tensor parallelism
python build.py \
    --world_size 4 \
    --tp_size 4 \
    --pp_size 1 \
    --model_dir 01-ai/Yi-34B \
    --dtype bfloat16 \
    --log_level info \
    --output_dir trtllm_models/yi-34b_tp4_pp1 \
    --max_input_len 1024 \
    --max_output_len 512 \
    --remove_input_padding \
    --use_gpt_attention_plugin bfloat16 \
    --use_gemm_plugin bfloat16 \
    --enable_context_fmha \
    --parallel_build \
    --use_parallel_embedding \
    --use_fused_mlp
```
### Run

To run a TensorRT-LLM Yi model using the engines generated by build.py :

```bash
# run Yi-6B with single GPU device
python run.py \
    --max_output_len 100 \
    --log_level info \
    --engine_dir trtllm_models/yi-6b_tp1_pp1 \
    --tokenizer_dir 01-ai/Yi-6B \
    --input_text "1,2,3,4,5,6,7,"

# run Yi-34B with 4 GPU devices (tensor_parallel=4)
mpirun -n 4 --allow-run-as-root \
    python run.py \
        --max_output_len 100 \
        --log_level info \
        --engine_dir trtllm_models/yi-34b_tp4_pp1 \
        --tokenizer_dir 01-ai/Yi-34B \
        --input_text "1,2,3,4,5,6,7,"
