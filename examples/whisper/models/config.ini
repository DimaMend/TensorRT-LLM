[encoder]
n_layer = 4
n_head = 6
vocab_size = 51864
n_state = 384
n_ctx = 1500
ffn_hidden_size = 1536
n_positions = 1024
has_position_embedding = False
has_token_type_embedding = False
has_embedding_layernorm = False
has_embedding_scale = False
q_scaling = 1.1547005383792517
has_attention_qkvo_bias = True
has_mlp_bias = True
has_model_final_layernorm = True
layernorm_eps = 1e-5
layernorm_position = pre_layernorm
layernorm_type = LayerNorm
hidden_act = gelu
relative_attention = False
storage_dtype = float32