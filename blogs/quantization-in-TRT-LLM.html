<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Speed up inference with SOTA quantization techniques in TRT-LLM &mdash; tensorrt_llm  documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=19f00094" />

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../_static/jquery.js?v=5d32c60e"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../_static/documentation_options.js?v=5929fcd5"></script>
        <script src="../_static/doctools.js?v=888ff710"></script>
        <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="Falcon-180B on a single H200 GPU with INT4 AWQ, and 6.7x faster Llama-70B over A100" href="Falcon180B-H200.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            tensorrt_llm
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../architecture.html">TensorRT-LLM Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpt_runtime.html">C++ GPT Runtime</a></li>
<li class="toctree-l1"><a class="reference internal" href="../batch_manager.html">The Batch Manager in TensorRT-LLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../inference_request.html">Inference Request</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gpt_attention.html">Multi-head, Multi-query and Group-query Attention</a></li>
<li class="toctree-l1"><a class="reference internal" href="../precision.html">Numerical Precision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../build_from_source.html">Build from Source</a></li>
<li class="toctree-l1"><a class="reference internal" href="../performance.html">Performance of TensorRT-LLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../2023-05-19-how-to-debug.html">How to debug</a></li>
<li class="toctree-l1"><a class="reference internal" href="../2023-05-17-how-to-add-a-new-model.html">How to add a new model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../graph-rewriting.html">Graph Rewriting Module</a></li>
<li class="toctree-l1"><a class="reference internal" href="../memory.html">Memory Usage of TensorRT-LLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../new_workflow.html">New Workflow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../lora.html">Run gpt-2b + LoRA using GptManager / cpp runtime</a></li>
<li class="toctree-l1"><a class="reference internal" href="../perf_best_practices.html">Best Practices for Tuning the Performance of TensorRT-LLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../performance_analysis.html">Performance Analysis of TensorRT-LLM</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../python-api/tensorrt_llm.layers.html">Layers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../python-api/tensorrt_llm.functional.html">Functionals</a></li>
<li class="toctree-l1"><a class="reference internal" href="../python-api/tensorrt_llm.models.html">Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../python-api/tensorrt_llm.plugin.html">Plugin</a></li>
<li class="toctree-l1"><a class="reference internal" href="../python-api/tensorrt_llm.quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../python-api/tensorrt_llm.runtime.html">Runtime</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">C++ API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../_cpp_gen/runtime.html">Runtime</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Blogs</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="H100vsA100.html">H100 has 4.6x A100 Performance in TensorRT-LLM, achieving 10,000 tok/s at 100ms to first token</a></li>
<li class="toctree-l1"><a class="reference internal" href="H200launch.html">H200 achieves nearly 12,000 tokens/sec on Llama2-13B with TensorRT-LLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="Falcon180B-H200.html">Falcon-180B on a single H200 GPU with INT4 AWQ, and 6.7x faster Llama-70B over A100</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Speed up inference with SOTA quantization techniques in TRT-LLM</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#quantization-in-tensorrt-llm">Quantization in TensorRT-LLM</a></li>
<li class="toctree-l2"><a class="reference internal" href="#benchmark">Benchmark</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#performance">Performance</a></li>
<li class="toctree-l3"><a class="reference internal" href="#accuracy">Accuracy</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#best-practices-to-choose-the-right-quantization-methods">Best practices to choose the right quantization methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="#whats-coming-next">What’s coming next</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">tensorrt_llm</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Speed up inference with SOTA quantization techniques in TRT-LLM</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/blogs/quantization-in-TRT-LLM.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="speed-up-inference-with-sota-quantization-techniques-in-trt-llm">
<h1>Speed up inference with SOTA quantization techniques in TRT-LLM<a class="headerlink" href="#speed-up-inference-with-sota-quantization-techniques-in-trt-llm" title="Link to this heading"></a></h1>
<p>The deployment and inference speed of LLMs are often impeded by limitations in memory capacity, memory bandwidth, and computation power. Quantization emerges as a vital strategy to address these bottlenecks, involving representing weights and activations with lower-precision data types like <a class="reference external" href="https://www.nvidia.com/en-us/on-demand/session/gtcspring23-s52166/">FP8</a>.</p>
<p>In this blog, we provide an overview of the quantization features in TensorRT-LLM, share benchmark, and offer best practices of selecting the appropriate quantization methods tailored to your specific use case.</p>
<section id="quantization-in-tensorrt-llm">
<h2>Quantization in TensorRT-LLM<a class="headerlink" href="#quantization-in-tensorrt-llm" title="Link to this heading"></a></h2>
<p>TensorRT-LLM offers a best-in-class unified quantization toolkit to significantly speedup DL/GenAI deployment on NVIDIA hardware, while maintaining model accuracy. This toolkit is designed with easy-of-use in mind. You can follow <a class="reference external" href="https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/quantization">this user guide</a> to quantize <a class="reference external" href="https://nvidia.github.io/TensorRT-LLM/precision.html#support-matrix">supported LLMs</a> with a few lines of codes. We currently focus on providing SOTA <strong>Post-Training Quantization (PTQ)</strong> and will soon expand to more model optimization techniques in the near future.</p>
</section>
<section id="benchmark">
<h2>Benchmark<a class="headerlink" href="#benchmark" title="Link to this heading"></a></h2>
<section id="performance">
<h3>Performance<a class="headerlink" href="#performance" title="Link to this heading"></a></h3>
<p>In the following benchmark, we highlight the acceleration of a few popular models at a small batch size without imposing latency constraints. It’s important to note that in scenarios where there’s a latency constraint in your application, TRT-LLM can achieve an even greater performance improvement. Using LLaMA-v2-7B as an example, when the first token latency is constrained to be under 500ms, quantization with FP8 and a batch size of 16 achieves a notable <strong>2.3x inference speedup</strong> compared to FP16 on a H100.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Model</p></th>
<th class="head text-center"><p>Batch Size</p></th>
<th class="head text-center"><p>Speedup (FP8 v.s. FP16)</p></th>
<th class="head text-center"><p>Speedup (INT8 SQ v.s. FP16)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>GPT-J</p></td>
<td class="text-center"><p>1</p></td>
<td class="text-center"><p>1.40x</p></td>
<td class="text-center"><p>1.40x</p></td>
</tr>
<tr class="row-odd"><td><p>GPT-J</p></td>
<td class="text-center"><p>8</p></td>
<td class="text-center"><p>1.44x</p></td>
<td class="text-center"><p>1.30x</p></td>
</tr>
<tr class="row-even"><td><p>LLaMA-v2-7B</p></td>
<td class="text-center"><p>1</p></td>
<td class="text-center"><p>1.51x</p></td>
<td class="text-center"><p>1.47x</p></td>
</tr>
<tr class="row-odd"><td><p>LLaMA-v2-7B</p></td>
<td class="text-center"><p>8</p></td>
<td class="text-center"><p>1.40x</p></td>
<td class="text-center"><p>1.32x</p></td>
</tr>
</tbody>
</table>
<p>*The above benchmarks were run with Input Length=1024, Output Length=128, and TP=1 on H100 80GB.</p>
</section>
<section id="accuracy">
<h3>Accuracy<a class="headerlink" href="#accuracy" title="Link to this heading"></a></h3>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Model</p></th>
<th class="head text-center"><p>Quantization Methods</p></th>
<th class="head text-center"><p>MMLU Baseline (FP16)</p></th>
<th class="head text-center"><p>MMLU Post-quantization</p></th>
<th class="head text-center"><p>MMLU Loss</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Falcon-180B</p></td>
<td class="text-center"><p>FP8</p></td>
<td class="text-center"><p>70.4</p></td>
<td class="text-center"><p>70.3</p></td>
<td class="text-center"><p>0.14%</p></td>
</tr>
<tr class="row-odd"><td><p></p></td>
<td class="text-center"><p>INT8-SQ</p></td>
<td class="text-center"><p>70.4</p></td>
<td class="text-center"><p>68.6</p></td>
<td class="text-center"><p>2.56%</p></td>
</tr>
<tr class="row-even"><td><p></p></td>
<td class="text-center"><p>INT4-AWQ</p></td>
<td class="text-center"><p>70.4</p></td>
<td class="text-center"><p>69.8</p></td>
<td class="text-center"><p>0.85%</p></td>
</tr>
<tr class="row-odd"><td><p>Falcon-40B</p></td>
<td class="text-center"><p>FP8</p></td>
<td class="text-center"><p>56.1</p></td>
<td class="text-center"><p>55.6</p></td>
<td class="text-center"><p>0.89%</p></td>
</tr>
<tr class="row-even"><td><p></p></td>
<td class="text-center"><p>INT8-SQ</p></td>
<td class="text-center"><p>56.1</p></td>
<td class="text-center"><p>54.7</p></td>
<td class="text-center"><p>2.50%</p></td>
</tr>
<tr class="row-odd"><td><p></p></td>
<td class="text-center"><p>INT4-AWQ</p></td>
<td class="text-center"><p>56.1</p></td>
<td class="text-center"><p>55.5</p></td>
<td class="text-center"><p>1.07%</p></td>
</tr>
<tr class="row-even"><td><p>LLaMA-v2-70B</p></td>
<td class="text-center"><p>FP8</p></td>
<td class="text-center"><p>69.1</p></td>
<td class="text-center"><p>68.5</p></td>
<td class="text-center"><p>0.87%</p></td>
</tr>
<tr class="row-odd"><td><p></p></td>
<td class="text-center"><p>INT8-SQ</p></td>
<td class="text-center"><p>69.1</p></td>
<td class="text-center"><p>67.2</p></td>
<td class="text-center"><p>2.75%</p></td>
</tr>
<tr class="row-even"><td><p></p></td>
<td class="text-center"><p>INT4-AWQ</p></td>
<td class="text-center"><p>69.1</p></td>
<td class="text-center"><p>68.4</p></td>
<td class="text-center"><p>1.01%</p></td>
</tr>
<tr class="row-odd"><td><p>MPT-30B</p></td>
<td class="text-center"><p>FP8</p></td>
<td class="text-center"><p>47.5</p></td>
<td class="text-center"><p>47.4</p></td>
<td class="text-center"><p>0.21%</p></td>
</tr>
<tr class="row-even"><td><p></p></td>
<td class="text-center"><p>INT8-SQ</p></td>
<td class="text-center"><p>47.5</p></td>
<td class="text-center"><p>46.8</p></td>
<td class="text-center"><p>1.47%</p></td>
</tr>
<tr class="row-odd"><td><p></p></td>
<td class="text-center"><p>INT4-AWQ</p></td>
<td class="text-center"><p>47.5</p></td>
<td class="text-center"><p>46.5</p></td>
<td class="text-center"><p>2.11%</p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="best-practices-to-choose-the-right-quantization-methods">
<h2>Best practices to choose the right quantization methods<a class="headerlink" href="#best-practices-to-choose-the-right-quantization-methods" title="Link to this heading"></a></h2>
<p>A quantization method comprises three primary components:</p>
<ol class="arabic simple">
<li><p>Weight precision format</p></li>
<li><p>Activation precision format</p></li>
<li><p>Calibration algorithms</p></li>
</ol>
<p>Typically, in the context of small-batch inference scenarios (batch size ≤ 4), the key consideration is memory bandwidth, making weight-only quantization methods the preferred choice. Conversely, for large-batch inference scenarios, such as serving scenarios (batch size ≥ 16), both memory bandwidth and computation density become crucial factors. Consequently, it’s recommended to opt for a quantization method that has both weight and activation quantized. For batch size ≥ 16, the choice of quantization method can be model speicfic. We suggest to prioritize using FP8 first, as we typically see it offers the best performance and accuracy. If the results do not meet your specific use case, you can further experiment with Int8 SmoothQuant (Int8 SQ) followed by AWQ and/or GPTQ.</p>
<p>Based on specific use cases, users might have different tolerances on accuracy impact and calibration time. The table below summarizes the tradeoffs* to consider when choosing a quantization method. You can also learn more about precision formats in our <a class="reference external" href="https://nvidia.github.io/TensorRT-LLM/precision.html">documentation</a>.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Quantization Methods</p></th>
<th class="head text-center"><p>Performance Improvement (batch size &lt;= 4)</p></th>
<th class="head text-center"><p>Performance Improvement (batch size &gt;= 16)</p></th>
<th class="head text-center"><p>Accuracy Impact</p></th>
<th class="head text-center"><p>Calibration Time**</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>FP8 (W8A8)</p></td>
<td class="text-center"><p>Medium</p></td>
<td class="text-center"><p>Medium</p></td>
<td class="text-center"><p>Very Low</p></td>
<td class="text-center"><p>Minutes</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Int8 SQ (W8A8)</p></td>
<td class="text-center"><p>Medium</p></td>
<td class="text-center"><p>Medium</p></td>
<td class="text-center"><p>Medium</p></td>
<td class="text-center"><p>Minutes</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>Int8 weight-only (W8A16)</p></td>
<td class="text-center"><p>Medium</p></td>
<td class="text-center"><p>Low</p></td>
<td class="text-center"><p>Low</p></td>
<td class="text-center"><p>Not Required</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Int4 weight-only (W4A16)</p></td>
<td class="text-center"><p>High</p></td>
<td class="text-center"><p>Low</p></td>
<td class="text-center"><p>High</p></td>
<td class="text-center"><p>Not Required</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>Int4 AWQ (W4A16)</p></td>
<td class="text-center"><p>High</p></td>
<td class="text-center"><p>Low</p></td>
<td class="text-center"><p>Low</p></td>
<td class="text-center"><p>Tens of Minutes</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Int4 GPTQ</p></td>
<td class="text-center"><p>High</p></td>
<td class="text-center"><p>Low</p></td>
<td class="text-center"><p>Low</p></td>
<td class="text-center"><p>Tens of Minutes</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>Int4-FP8 AWQ (W4A8)</p></td>
<td class="text-center"><p>High</p></td>
<td class="text-center"><p>Medium</p></td>
<td class="text-center"><p>Low</p></td>
<td class="text-center"><p>Tens of Minutes</p></td>
</tr>
</tbody>
</table>
<p>* The performance and impact are measured on 10+ popular LLMs. We’ll follow up with more data points.
** Calibration time is subject to the actual model size.</p>
<p>We note that TensorRT-LLM also offers INT8 and FP8 quantization for KV cache. KV cache differs from normal activation because it occupies non-negligible persistent memory under scenarios like large batch sizes or long context lengths. If you’re using KV cache on Hopper &amp; Ada GPUs, We recommend using FP8 KV cache over Int8 because the former has a lower accuracy impact than the latter in most tested cases. When switching from FP16 KV cache to FP8 KV cache, it also enables you to run 2-3x larger batch size on H100 machine for models like GPT-J which further brings about 1.5x performance benefit.</p>
</section>
<section id="whats-coming-next">
<h2>What’s coming next<a class="headerlink" href="#whats-coming-next" title="Link to this heading"></a></h2>
<p>TensorRT-LLM continues to make improvements on our quantization features, such as Int4-FP8 AWQ (W4A8) public examples and more model supports. Please stay tuned for our upcoming releases.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="Falcon180B-H200.html" class="btn btn-neutral float-left" title="Falcon-180B on a single H200 GPU with INT4 AWQ, and 6.7x faster Llama-70B over A100" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, NVidia.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>